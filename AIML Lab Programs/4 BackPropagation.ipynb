{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48332003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted Output: \n",
      "[[0.46794265]\n",
      " [0.45452134]\n",
      " [0.43953597]]\n",
      "\n",
      "Loss: \n",
      "0.1905622100435118\n"
     ]
    }
   ],
   "source": [
    "# CSE \n",
    "import numpy as np\n",
    "\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)     # X = (hours sleeping, hours studying)\n",
    "y = np.array(([92], [86], [89]), dtype=float)           # y = score on test\n",
    "\n",
    "# scale units\n",
    "X = X/np.amax(X, axis=0)        # maximum of X array\n",
    "y = y/100                       # max test score is 100\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "                            # Parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "                             # Weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize)        # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize)       # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "                             #forward propagation through our network\n",
    "        self.z = np.dot(X, self.W1)               # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z)            # activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2)        # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3)                 # final activation function\n",
    "        return o \n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))     # activation function \n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)          # derivative of sigmoid\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "                                    # backward propgate through the network\n",
    "        self.o_error = y - o        # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to \n",
    "        self.z2_error = self.o_delta.dot(self.W2.T)    # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "        self.W1 += X.T.dot(self.z2_delta)       # adjusting first set (input --> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(self.o_delta)  # adjusting second set (hidden --> output) weights\n",
    "\n",
    "    def train (self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "NN = Neural_Network()\n",
    "print (\"\\nInput: \\n\" + str(X))\n",
    "print (\"\\nActual Output: \\n\" + str(y)) \n",
    "print (\"\\nPredicted Output: \\n\" + str(NN.forward(X)))\n",
    "print (\"\\nLoss: \\n\" + str(np.mean(np.square(y - NN.forward(X)))))     # mean sum squared loss)\n",
    "NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e758f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Sale</th>\n",
       "      <th>Predicted Sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.1</td>\n",
       "      <td>20.496948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.4</td>\n",
       "      <td>10.536427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.3</td>\n",
       "      <td>10.196403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>17.254924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.9</td>\n",
       "      <td>11.831302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>7.6</td>\n",
       "      <td>7.369535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9.7</td>\n",
       "      <td>10.187097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>12.8</td>\n",
       "      <td>12.606203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>25.5</td>\n",
       "      <td>23.733581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>13.4</td>\n",
       "      <td>14.755203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual Sale  Predicted Sale\n",
       "0           22.1       20.496948\n",
       "1           10.4       10.536427\n",
       "2            9.3       10.196403\n",
       "3           18.5       17.254924\n",
       "4           12.9       11.831302\n",
       "..           ...             ...\n",
       "195          7.6        7.369535\n",
       "196          9.7       10.187097\n",
       "197         12.8       12.606203\n",
       "198         25.5       23.733581\n",
       "199         13.4       14.755203\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AIML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sales = pd.read_csv(\"Advertising.csv\")\n",
    "\n",
    "X = sales[[\"TV\", \"Radio\", \"Newspaper\"]]\n",
    "y = sales[\"Sales\"]\n",
    "\n",
    "# Standardizes features\n",
    "X = X.apply(lambda x: (x - X.mean()) / X.std(), axis = 1)\n",
    "\n",
    "# Scaling target variable\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_transformed = y_scaler.fit_transform(np.reshape(y, (-1,1)))\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Returns sigmoid value for the input parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Returns derivative of sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Count of units in the input layer; The count is equal to the number of features in data set\n",
    "input_layer_units = X.shape[1]\n",
    "\n",
    "# number of units at output layer; It is one for target being a continous variable\n",
    "output_layer_units = 1\n",
    "\n",
    "\n",
    "# Hyperparameters initialization\n",
    "\n",
    "# Number of times training data will be used for model training\n",
    "epoch = 5000\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Count of units in the hidden layer; It was assumed that there will be just one hidden layer\n",
    "hidden_layer_units = 3\n",
    "\n",
    "\n",
    "# Weights from input layer nodes to hidden layer nodes\n",
    "hidden_layer_weights = np.random.uniform(size=(input_layer_units, hidden_layer_units))\n",
    "\n",
    "# Biases for hidden layer nodes\n",
    "hidden_layer_biases = np.random.uniform(size=(1, hidden_layer_units))\n",
    "                                         \n",
    "# Weights from hidden layer nodes to output layer nodes\n",
    "output_layer_weights = np.random.uniform(size=(hidden_layer_units,output_layer_units))\n",
    "\n",
    "# Biases for output layer nodes\n",
    "output_layer_biases=np.random.uniform(size=(1,output_layer_units))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    #Forward Propogation\n",
    "    hidden_layer_nets = np.dot(X, hidden_layer_weights)\n",
    "    hidden_layer_nets = hidden_layer_nets + hidden_layer_biases\n",
    "    hidden_layer_outputs = sigmoid(hidden_layer_nets)\n",
    "    \n",
    "    output_layer_nets = np.dot(hidden_layer_outputs, output_layer_weights)\n",
    "    output_layer_nets = output_layer_nets + output_layer_biases\n",
    "    output = sigmoid(output_layer_nets)\n",
    "\n",
    "    #Backpropagation\n",
    "    output_error = y_transformed - output\n",
    "    output_gradients = sigmoid_derivative(output)\n",
    "    output_delta = output_error * output_gradients\n",
    "    hidden_layer_error = output_delta.dot(output_layer_weights.T)\n",
    "\n",
    "    # Calculation of hidden layer weights' contribution to error\n",
    "    hidden_layer_gradients = sigmoid_derivative(hidden_layer_outputs)\n",
    "    hidden_layer_delta = hidden_layer_error * hidden_layer_gradients\n",
    "\n",
    "    # Weights updates for both output and hidden layer units\n",
    "    output_layer_weights += learning_rate * hidden_layer_outputs.T.dot(output_delta)\n",
    "    hidden_layer_weights += learning_rate * X.T.dot(hidden_layer_delta)\n",
    "\n",
    "# Transforms data back from scaled ones\n",
    "predictions = y_scaler.inverse_transform(output)\n",
    "\n",
    "\n",
    "# Shows the predicted sales against actual sale for all data points\n",
    "pd.DataFrame({\"Actual Sale\": y, \"Predicted Sale\": predictions.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94f9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.87378388]\n",
      " [0.85341593]\n",
      " [0.87406796]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "X = X/np.amax(X,axis=0)\n",
    "y = y/100\n",
    "print(X)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "epoch=5000 \t\n",
    "lr=0.1 \t\t\n",
    "inputlayer_neurons = 2 \t\t\n",
    "hiddenlayer_neurons = 3 \t\n",
    "output_neurons = 1\n",
    "\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "    hinp1=np.dot(X,wh)\n",
    "    hinp=hinp1 + bh\n",
    "    hlayer_act = sigmoid(hinp)\n",
    "    outinp1=np.dot(hlayer_act,wout)\n",
    "    outinp= outinp1+ bout\n",
    "    output = sigmoid(outinp)\n",
    "    n= 0\n",
    "    EO = y-output\n",
    "    outgrad = derivatives_sigmoid(output)\n",
    "    d_output = EO* outgrad\n",
    "    EH = d_output.dot(wout.T)\n",
    "    hiddengrad = derivatives_sigmoid(hlayer_act)\n",
    "    d_hiddenlayer = EH * hiddengrad\n",
    "\n",
    "wout += hlayer_act.T.dot(d_output) *lr\n",
    "wh += X.T.dot(d_hiddenlayer) *lr\n",
    "n+=1\n",
    "print(\"Input: \\n\" + str(X)) \n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" ,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9f0707",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.04732496 -0.76056057]]\n",
      "###output######## [[0.95267504 0.76056057]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.04731239 -0.75597495]]\n",
      "###output######## [[0.95268761 0.75597495]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.04729963 -0.75129891]]\n",
      "###output######## [[0.95270037 0.75129891]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.04728667 -0.74653278]]\n",
      "###output######## [[0.95271333 0.74653278]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.04727351 -0.74167711]]\n",
      "###output######## [[0.95272649 0.74167711]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.04726014 -0.73673267]]\n",
      "###output######## [[0.95273986 0.73673267]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.04724655 -0.73170042]]\n",
      "###output######## [[0.95275345 0.73170042]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.04723273 -0.7265816 ]]\n",
      "###output######## [[0.95276727 0.7265816 ]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.04721868 -0.72137763]]\n",
      "###output######## [[0.95278132 0.72137763]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.0472044 -0.7160902]]\n",
      "###output######## [[0.9527956 0.7160902]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.04718987 -0.71072125]]\n",
      "###output######## [[0.95281013 0.71072125]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.04717509 -0.70527295]]\n",
      "###output######## [[0.95282491 0.70527295]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.04716005 -0.69974775]]\n",
      "###output######## [[0.95283995 0.69974775]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.04714476 -0.69414831]]\n",
      "###output######## [[0.95285524 0.69414831]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.0471292  -0.68847757]]\n",
      "###output######## [[0.9528708  0.68847757]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.04711338 -0.68273869]]\n",
      "###output######## [[0.95288662 0.68273869]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.04709729 -0.67693508]]\n",
      "###output######## [[0.95290271 0.67693508]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.04708093 -0.67107037]]\n",
      "###output######## [[0.95291907 0.67107037]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.0470643  -0.66514841]]\n",
      "###output######## [[0.9529357  0.66514841]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.0470474  -0.65917325]]\n",
      "###output######## [[0.9529526  0.65917325]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.04703022 -0.65314913]]\n",
      "###output######## [[0.95296978 0.65314913]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.04701277 -0.64708047]]\n",
      "###output######## [[0.95298723 0.64708047]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.04699505 -0.64097184]]\n",
      "###output######## [[0.95300495 0.64097184]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.04697706 -0.63482794]]\n",
      "###output######## [[0.95302294 0.63482794]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.04695881 -0.62865361]]\n",
      "###output######## [[0.95304119 0.62865361]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.04694029 -0.62245375]]\n",
      "###output######## [[0.95305971 0.62245375]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.04692151 -0.61623335]]\n",
      "###output######## [[0.95307849 0.61623335]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.04690248 -0.60999746]]\n",
      "###output######## [[0.95309752 0.60999746]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.04688319 -0.60375112]]\n",
      "###output######## [[0.95311681 0.60375112]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.04686366 -0.59749941]]\n",
      "###output######## [[0.95313634 0.59749941]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.04684389 -0.59124735]]\n",
      "###output######## [[0.95315611 0.59124735]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.04682389 -0.58499994]]\n",
      "###output######## [[0.95317611 0.58499994]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.04680367 -0.57876211]]\n",
      "###output######## [[0.95319633 0.57876211]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.04678323 -0.57253869]]\n",
      "###output######## [[0.95321677 0.57253869]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.04676257 -0.5663344 ]]\n",
      "###output######## [[0.95323743 0.5663344 ]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.04674172 -0.56015385]]\n",
      "###output######## [[0.95325828 0.56015385]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.04672067 -0.55400148]]\n",
      "###output######## [[0.95327933 0.55400148]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.04669944 -0.54788159]]\n",
      "###output######## [[0.95330056 0.54788159]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.04667803 -0.54179829]]\n",
      "###output######## [[0.95332197 0.54179829]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.04665646 -0.53575552]]\n",
      "###output######## [[0.95334354 0.53575552]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.04663472 -0.529757  ]]\n",
      "###output######## [[0.95336528 0.529757  ]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.04661284 -0.52380627]]\n",
      "###output######## [[0.95338716 0.52380627]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.04659082 -0.51790664]]\n",
      "###output######## [[0.95340918 0.51790664]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.04656866 -0.5120612 ]]\n",
      "###output######## [[0.95343134 0.5120612 ]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.04654639 -0.50627285]]\n",
      "###output######## [[0.95345361 0.50627285]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.046524   -0.50054423]]\n",
      "###output######## [[0.953476   0.50054423]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.04650151 -0.49487778]]\n",
      "###output######## [[0.95349849 0.49487778]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.04647892 -0.48927572]]\n",
      "###output######## [[0.95352108 0.48927572]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.04645625 -0.48374005]]\n",
      "###output######## [[0.95354375 0.48374005]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.0464335  -0.47827256]]\n",
      "###output######## [[0.9535665  0.47827256]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.01936719 -0.02183002]]\n",
      "###output######## [[0.98063281 0.02183002]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.01936582 -0.02182806]]\n",
      "###output######## [[0.98063418 0.02182806]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.01936446 -0.02182611]]\n",
      "###output######## [[0.98063554 0.02182611]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.0193631  -0.02182416]]\n",
      "###output######## [[0.9806369  0.02182416]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.01936174 -0.02182221]]\n",
      "###output######## [[0.98063826 0.02182221]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.01936037 -0.02182025]]\n",
      "###output######## [[0.98063963 0.02182025]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.01935901 -0.0218183 ]]\n",
      "###output######## [[0.98064099 0.0218183 ]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.01935765 -0.02181635]]\n",
      "###output######## [[0.98064235 0.02181635]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.01935629 -0.0218144 ]]\n",
      "###output######## [[0.98064371 0.0218144 ]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.01935493 -0.02181245]]\n",
      "###output######## [[0.98064507 0.02181245]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.01935357 -0.02181051]]\n",
      "###output######## [[0.98064643 0.02181051]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.01935221 -0.02180856]]\n",
      "###output######## [[0.98064779 0.02180856]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.01935085 -0.02180661]]\n",
      "###output######## [[0.98064915 0.02180661]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.01934949 -0.02180466]]\n",
      "###output######## [[0.98065051 0.02180466]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.01934813 -0.02180272]]\n",
      "###output######## [[0.98065187 0.02180272]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.01934677 -0.02180077]]\n",
      "###output######## [[0.98065323 0.02180077]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.01934541 -0.02179882]]\n",
      "###output######## [[0.98065459 0.02179882]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.01934405 -0.02179688]]\n",
      "###output######## [[0.98065595 0.02179688]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.01934269 -0.02179493]]\n",
      "###output######## [[0.98065731 0.02179493]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.01934133 -0.02179299]]\n",
      "###output######## [[0.98065867 0.02179299]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.01933998 -0.02179105]]\n",
      "###output######## [[0.98066002 0.02179105]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.01933862 -0.0217891 ]]\n",
      "###output######## [[0.98066138 0.0217891 ]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.01933726 -0.02178716]]\n",
      "###output######## [[0.98066274 0.02178716]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.0193359  -0.02178522]]\n",
      "###output######## [[0.9806641  0.02178522]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.01933455 -0.02178328]]\n",
      "###output######## [[0.98066545 0.02178328]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.01933319 -0.02178134]]\n",
      "###output######## [[0.98066681 0.02178134]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.01933183 -0.0217794 ]]\n",
      "###output######## [[0.98066817 0.0217794 ]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.01933048 -0.02177746]]\n",
      "###output######## [[0.98066952 0.02177746]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.01932912 -0.02177552]]\n",
      "###output######## [[0.98067088 0.02177552]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.01932777 -0.02177358]]\n",
      "###output######## [[0.98067223 0.02177358]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.01932641 -0.02177164]]\n",
      "###output######## [[0.98067359 0.02177164]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.01932506 -0.0217697 ]]\n",
      "###output######## [[0.98067494 0.0217697 ]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.0193237  -0.02176776]]\n",
      "###output######## [[0.9806763  0.02176776]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.01932235 -0.02176583]]\n",
      "###output######## [[0.98067765 0.02176583]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.01932099 -0.02176389]]\n",
      "###output######## [[0.98067901 0.02176389]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.01931964 -0.02176196]]\n",
      "###output######## [[0.98068036 0.02176196]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.01931829 -0.02176002]]\n",
      "###output######## [[0.98068171 0.02176002]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.01931693 -0.02175809]]\n",
      "###output######## [[0.98068307 0.02175809]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.01931558 -0.02175615]]\n",
      "###output######## [[0.98068442 0.02175615]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.01931423 -0.02175422]]\n",
      "###output######## [[0.98068577 0.02175422]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.01931288 -0.02175228]]\n",
      "###output######## [[0.98068712 0.02175228]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.01931153 -0.02175035]]\n",
      "###output######## [[0.98068847 0.02175035]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.01931017 -0.02174842]]\n",
      "###output######## [[0.98068983 0.02174842]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.01930882 -0.02174649]]\n",
      "###output######## [[0.98069118 0.02174649]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.01930747 -0.02174456]]\n",
      "###output######## [[0.98069253 0.02174456]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.01930612 -0.02174262]]\n",
      "###output######## [[0.98069388 0.02174262]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.01930477 -0.02174069]]\n",
      "###output######## [[0.98069523 0.02174069]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.01930342 -0.02173876]]\n",
      "###output######## [[0.98069658 0.02173876]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.01930207 -0.02173683]]\n",
      "###output######## [[0.98069793 0.02173683]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputNeurons=2 \n",
    "hiddenlayerNeurons=4 \n",
    "outputNeurons=2 \n",
    "iteration=6000\n",
    "\n",
    "input = np.random.randint(1,5,inputNeurons) \n",
    "output = np.array([1.0,0.0]) \n",
    "hidden_layer=np.random.rand(1,hiddenlayerNeurons)\n",
    "\n",
    "hidden_biass=np.random.rand(1,hiddenlayerNeurons) \n",
    "output_bias=np.random.rand(1,outputNeurons) \n",
    "hidden_weights=np.random.rand(inputNeurons,hiddenlayerNeurons) \n",
    "output_weights=np.random.rand(hiddenlayerNeurons,outputNeurons)\n",
    "\n",
    "def sigmoid (layer):\n",
    "    return 1/(1 + np.exp(-layer))\n",
    "\n",
    "def gradient(layer): \n",
    "    return layer*(1-layer)\n",
    "\n",
    "for i in range(iteration):\n",
    "\n",
    "    hidden_layer=np.dot(input,hidden_weights) \n",
    "    hidden_layer=sigmoid(hidden_layer+hidden_biass)\n",
    "\n",
    "    output_layer=np.dot(hidden_layer,output_weights) \n",
    "    output_layer=sigmoid(output_layer+output_bias)\n",
    "\n",
    "    error = (output-output_layer) \n",
    "    gradient_outputLayer=gradient(output_layer)\n",
    "    error_terms_output=gradient_outputLayer * error \n",
    "    error_terms_hidden=gradient(hidden_layer)*np.dot(error_terms_output,output_weights.T)\n",
    "\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons,1),error_terms_hidden.reshape(1,hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons,1),error_terms_output.reshape(1,outputNeurons))\n",
    "\n",
    "    hidden_weights = hidden_weights + 0.05*gradient_hidden_weights \n",
    "    output_weights = output_weights + 0.05*gradient_ouput_weights \n",
    "    if i<50 or i>iteration-50:\n",
    "        print(\"**********************\") \n",
    "        print(\"iteration:\",i,\"::::\",error) \n",
    "        print(\"###output########\",output_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
